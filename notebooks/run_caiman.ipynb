{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3881c4",
   "metadata": {},
   "source": [
    "# Running the CaImAn pipeline with DataJoint\n",
    "\n",
    "This notebook provides the necessary steps to run the CaImAn segmentation pipeline on the DataJoint database. \n",
    "\n",
    "<b>IMPORTANT: </b> This notebook assumes that all user-specific data (for `common_exp.Session()`, `common_img.Scan()` and `common_img.RawImagingFile()`) have been entered elsewhere! Once this is done, the pipeline is common for all useres and can be executed here.\n",
    "\n",
    "Because this notebook is using CaImAn's functions, it needs to be run inside the Caiman environment. If you are in the Datajoint environment, the tables will still load and you can view the data, but trying to call Caiman-specific functions will result in `ModuleNotFoundErrors` or `NameErrors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f038c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting hheise@130.60.53.48:3306\n"
     ]
    }
   ],
   "source": [
    "# Importing datajoint tables\n",
    "import sys\n",
    "sys.path.append(\"..\\\\\")\n",
    "\n",
    "import login\n",
    "login.connect()\n",
    "from schema import common_img, common_exp\n",
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2406337",
   "metadata": {},
   "source": [
    "## Step 0: Check that the manual data is there\n",
    "\n",
    "Before running the actual Caiman pipeline, the manual data has to be entered into the database so that Datajoint knows where to start looking for the data. You can do this by checking the `Session()` and `Scan()` tables, with restrictions to only get the sessions you are about to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the query with some parameters\n",
    "restrictions = dict(username='hheise', mouse_id=69)\n",
    "\n",
    "# Show the entries of the tables that fit these restrictions\n",
    "(common_exp.Session() & restrictions & \"day = '2021-08-11'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c946733",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.Scan() & restrictions & \"day = '2021-07-16'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b76a1",
   "metadata": {},
   "source": [
    "## Run the pipeline: Populate Computed Tables\n",
    "\n",
    "If the above queries returned the right data, we are good to go for our pipeline!\n",
    "\n",
    "Datajoint's pipeline consists mainly of consecutive `Computed` tables, that can be filled in automatically with the processed data. This \"filling in\" is done by calling each table's `populate()` function. A detailed explanation can be found [here](https://docs.datajoint.org/python/computation/01-autopopulate.html#populate), but in short, Datajoint looks up all possible combinations of entries in upstream tables and runs it's `make()` function (where the actual computation for the table is located) for each of these combinations. This makes it possible for example to run and store the Caiman Segmentation for one session and several sets of parameters.\n",
    "\n",
    "Datajoint offers us a basic way to visualize the tables and its dependencies. The color of each table name shows its type (green = `Manual`, grey = `Lookup`, blue = `Imported`, red = `Computed`). The pipeline is visible as the \"string\" of red `Computed` tables going from `ScanInfo()` to `Deconvolution()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Entity Relationship Diagram (ERD) of a schema.\n",
    "### If the error '[WinError 2] \"dot\" not found in path.' occurs, you have to install graphviz by running \n",
    "### \"conda install -c conda-forge python-graphviz\" in an anaconda prompt with your environment active.\n",
    "%matplotlib qt\n",
    "dj.ERD(common_img).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a367ee",
   "metadata": {},
   "source": [
    "## Step 0.5: Populate RawImagingFile()\n",
    "\n",
    "Before running the actual pipeline, we have to find the raw imaging files from the current scan session. Datajoint does this automatically, but makes two assumptions that each user has to ensure is true:\n",
    "\n",
    "1. All imaging files for each session have to be located in the directory specified in `session_path` on the user's data directory on the Wahl server\n",
    "2. All imaging files have to follow the naming convention(s) specified in the user's `gui_params.yaml` file under the `scientifica_file` entry\n",
    "\n",
    "Datajoint then goes through the `session_path` directory and all its subfolders and finds all files that fit the pattern. The pipeline will then be performed on these files.\n",
    "\n",
    "**Important:** The database only stores the **file paths** of raw files, not the files themselves, due to memory constraints. It cannot keep track of changes to this file path after you entered the data into `RawImagingFile()`. This means that if you move the files to a different location, or rename one of the upstream folders, you will break the file path and Datajoint will not find the files anymore. In this case, if you want to perform processing steps where Datajoint needs access to the raw files (like motion correction or segmentation), you will have to update `session_path` of these files before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the table\n",
    "common_img.RawImagingFile().populate(display_progress=True)\n",
    "\n",
    "# View the resulting entry\n",
    "common_img.RawImagingFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638eb3b1",
   "metadata": {},
   "source": [
    "## Step 1: Populate ScanInfo()\n",
    "\n",
    "The first `Computed` table we have to populate is `ScanInfo()`. This table stores hardware and software settings of the scan that can be automatically read from the raw TIFF files coming from ScanImage, like frame rate, Pockels cell power, or XYZ stage position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the table\n",
    "common_img.ScanInfo().populate({'username': 'hheise'}, reserve_jobs=False, display_progress=True)\n",
    "\n",
    "# View the resulting entry\n",
    "common_img.ScanInfo() & restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86541ec0",
   "metadata": {},
   "source": [
    "## Step 2: Motion Correction\n",
    "\n",
    "Next up is the motion correction. Here we need a parameter set for the first time. The parameters for the motion correction is stored in the `Manual` table `MotionParameter()`. Because the motion correction parameters are generally transferable across brain regions and mice, we have a single table storing all sets. Each has a short description about the condition for which this set is appropriate (e.g. which Zoom setting, low or high movement artefacts, etc).\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.MotionParameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775b04b",
   "metadata": {},
   "source": [
    "If we want to add a parameter set, we can do this manually by entering a new dict into the table. This function also automatically saves manual each entry in a backup YAML file, in case the database gets corrupted and has to be restored.\n",
    "\n",
    "If you do not want to change each parameter, you can comment them out. In this case, the default value from `motion_id = 0` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580eff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_entry = dict(\n",
    "                 motion_id=0,\n",
    "                 motion_shortname='default',\n",
    "                 motion_description='Default values that work well from Hendriks previous experience. Uses piecewise-rigid correction.',\n",
    "                 crop_left=12,\n",
    "                 crop_right=12,\n",
    "                 offset=220,\n",
    "                 max_shift=50,\n",
    "                 stride_mc=150,\n",
    "                 overlap_mc=30,\n",
    "                 pw_rigid=1,\n",
    "                 max_dev_rigid=3,\n",
    "                 border_nan=0,\n",
    "                 n_iter_rig=2,\n",
    "                 nonneg_movie=1,\n",
    "                )\n",
    "\n",
    "common_img.MotionParameter().helper_insert1(new_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c1526",
   "metadata": {},
   "source": [
    "Once we have decided which parameter set to use, we can call the `populate()` function of the `MotionCorrection()` table with the respective restriction.\n",
    "\n",
    "`MotionCorrection()` does several things for each scan:\n",
    "1. Download the raw TIFF files for this scan into a temporary cache directory on the local machine.\n",
    "2. Do some preprocessing necessary for files from the H37R Scientifica: Align odd and even scan lines, crop the left and right borders to avoid edge artifacts, and apply a fixed offset (see `MotionParameter()` entry) to avoid mean negative pixel values that can mess with the CNMF algorithm.\n",
    "3. Perform the actual motion correction.\n",
    "4. Extract the calculated pixel shifts from the motion correction and store it in the database.\n",
    "5. Calculate and save some quality control metrics (e.g. correlation with template, means and standard deviations)\n",
    "6. Delete the memory-mapped files from the disk.\n",
    "\n",
    "As you can see, we delete the memory-mapped file directly after the motion correction. This is because we only store the pixel shifts in the database, and not the entire file. When we need the motion-corrected memory-mapped movie, we can recreate it easily by applying the pixel shifts to the corrected raw TIFF files. This is much faster than running the entire motion correction algorithm again, and it saves tons of disk space on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the populate() function, restricting it if necessary for only one set of parameters\n",
    "common_img.MotionCorrection().populate({'username': 'hheise', 'motion_id': 0, 'mouse_id': 69}, display_progress=True)\n",
    "\n",
    "# View the resulting entry\n",
    "common_img.MotionCorrection() & restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ecb18c",
   "metadata": {},
   "source": [
    "## Step 2.5: Quality control of MotionCorrection()\n",
    "\n",
    "To be able to quickly judge whether the motion correction was sufficiently effective without having to export the entire movie, we are computing some metrics that can help evaluate the motion correction. This table is not necessary for the next step of the pipeline, but is nice to have.\n",
    "\n",
    "These are Z-projections where each pixel has a certain value dependent on the time course of this pixel. The Z-projections we calculate are:\n",
    "- Average intensity\n",
    "- Local correlation (intensity correlation between each pixel and its 8 neighbors)\n",
    "- Standard deviation, minimum and maximum\n",
    "- 99.9th percentile\n",
    "- average intensity of all pixels over time (makes slow intensity changes visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35292074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_img.QualityControl().populate({'username': 'hheise'}, reserve_jobs=False, display_progress=True)\n",
    "\n",
    "# common_img.QualityControl() & restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f1cf4",
   "metadata": {},
   "source": [
    "## Step 3: Segmentation\n",
    "\n",
    "This is the actual CaImAn segmentation algorithm. It performs component detection, evaluation and dF/F computation steps. For this we also need a parameter set, which are stored in `CaimanParameter()`. Because these parameters are much more dependent on brain region and the specific FOV of the mouse (cell density, fluorescence intensity, etc.), each mouse gets its own list of parameter sets. If you want to use the same set of parameters for several mice, the set has to be entered into the database for each mouse. \n",
    "\n",
    "This means that instead of only one ID (like `motion_id`), `CaimanParameter()` needs two, `caiman_id` and `mouse_id`. This also means that several mice can have a set with `caiman_id=0`, but that contains slightly different parameters. This can be useful to store sets with different criteria. `caiman_id=0` can be a strict parameter set that only finds 100% certain neurons, and `caiman_id=1` could be more lenient, also accepting components that are not 100% neurons. The exact values in each set will vary between mice, but the \"meaning\" of each `caiman_id` is conserved across mice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b76a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.CaimanParameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc6898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_entry = dict(\n",
    "            # Identifiers\n",
    "             username = 'hheise',\n",
    "             mouse_id = 93,\n",
    "             caiman_id = 0,\n",
    "            # Component detection parameters\n",
    "             p = 1,\n",
    "             nb = 2,\n",
    "             merge_thr = 0.75,\n",
    "             rf = 25,\n",
    "             stride_cnmf = 6,\n",
    "             k = 18,\n",
    "             g_sig = 4,\n",
    "             method_init = 'greedy_roi',\n",
    "             ssub = 2,\n",
    "             tsub = 2,\n",
    "            # Evaluation parameters\n",
    "             snr_lowest = 5.0,\n",
    "             snr_thr = 9.0,\n",
    "             rval_lowest = -1.0,\n",
    "             rval_thr = 0.85,\n",
    "             cnn_lowest = 0.1,\n",
    "             cnn_thr = 0.9,\n",
    "            # dF/F parameters\n",
    "             flag_auto = 1,\n",
    "             quantile_min = 8,\n",
    "             frame_window = 2000,\n",
    "            )\n",
    "\n",
    "common_img.CaimanParameter().helper_insert1(new_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3968a5",
   "metadata": {},
   "source": [
    "The `Segmentation()` table performs the complete CaImAn pipeline. Results are not stored in an HDF5 file (although you can save them as such), but stored directly in the database. Each accepted component and their attributes (spatial and temporal components, dF/F, evaluation scores) are stored in the `Segmentation.ROI()` Part-table.\n",
    "\n",
    "`Segmentation().populate()` has two additional arguments, which control whether a FOV overview with outlined components should be saved after source detection and after evaluation (`save_overviews`) and whether the results should be stored in a traditional `cnm_results.hdf5` file in addition to the database entries (`save_results`). If `True`, these results are saved in the session folder on the Wahl server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d25f37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmentation: 100%|███████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 171.94it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'restrictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14196\\1469846350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcommon_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'username'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'hheise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'caiman_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_overviews\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreserve_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcommon_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mrestrictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'restrictions' is not defined"
     ]
    }
   ],
   "source": [
    "common_img.Segmentation().populate({'username': 'hheise', 'caiman_id': 0}, make_kwargs=dict(save_overviews=True, save_results=False), display_progress=True, suppress_errors=True, reserve_jobs=True)\n",
    "\n",
    "common_img.Segmentation() & restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.Segmentation.ROI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959377ac",
   "metadata": {},
   "source": [
    "## Step 4: Deconvolution\n",
    "\n",
    "The last step of our pipeline is Peter's CASCADE deconvolution. For this we need to decide which pretrained model to use. The best four are included in `DeconvolutionModel()`, with different parameters that influence the deconvolution [described here](https://github.com/HelmchenLabSoftware/Cascade#what-does-the-smoothing-parameter-for-the-models-mean):\n",
    "\n",
    "- Smoothing window: standard deviation of smoothing Gaussian kernel around ground truth in milliseconds. Depends on the frame rate of the recording. At 30 Hz, a smoothing of 50 ms is recommended. If imaging quality is bad, increase smoothing window.\n",
    "- Causal kernel: By default, the ground truth is smoothed symmetrically in time with a Gaussian kernel. This might predict some activity before the actual calcium transient. If time resolution is very important, e.g. with stimulus-triggered activity, a causal kernel almost exclusively predicts activity after the calcium event. However, a causal kernel is more error-prone and might predict events on noise, so should only be chosen if really necessary.\n",
    "\n",
    "Currently, four models are included in `DeconvolutionModel()`: 50ms, 100ms, causal kernel, Gaussian kernel. Additional models can be downloaded [here](https://github.com/HelmchenLabSoftware/Cascade/blob/master/Pretrained_models/available_models.yaml) and added to the table. In general, the ensemble models trained on many different datasets of excitatory activity (labeled `Global_EXC_`) perform better than single models and should be preferred.\n",
    "\n",
    "When populating `Deconvolution()`, restrict the algorithm to only use the most suitable model.\n",
    "\n",
    "The results of of the deconvolution are stored in the `Deconvolution.ROI()` part table again, with the same ID as in `Segmentation.ROI()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.DeconvolutionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d70e09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deconvolution:   0%|                                                                             | 0/6 [00:00<?, ?it/s]C:\\Anaconda3\\envs\\caiman\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Deconvolution for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 10), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0, 'decon_id': 1}\n",
      "Using deconvolution model Global_EXC_30Hz_smoothing50ms\n",
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 50 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 30 Hz\n",
      "Given argument traces contains 759 neurons and 36753 frames.\n",
      "Noise levels (mean, std; in standard units): 0.01, 0.0\n",
      "\n",
      "Predictions for noise level 2:\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\caiman\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\caiman\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t... ensemble 0\n",
      "27895527/27895527 [==============================] - 125s 4us/sample\n",
      "\t... ensemble 1\n",
      "27895527/27895527 [==============================] - 128s 5us/sample\n",
      "\t... ensemble 2\n",
      "27895527/27895527 [==============================] - 128s 5us/sample\n",
      "\t... ensemble 3\n",
      "27895527/27895527 [==============================] - 129s 5us/sample\n",
      "\t... ensemble 4\n",
      "27895527/27895527 [==============================] - 129s 5us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Deconvolution:  17%|███████████▎                                                        | 1/6 [11:32<57:41, 692.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Deconvolution for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 14), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0, 'decon_id': 1}\n",
      "Using deconvolution model Global_EXC_30Hz_smoothing50ms\n",
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 50 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 30 Hz\n",
      "Given argument traces contains 1251 neurons and 59095 frames.\n",
      "Noise levels (mean, std; in standard units): 1.07, 0.2\n",
      "\n",
      "Predictions for noise level 2:\n",
      "\t... ensemble 0\n",
      "73927845/73927845 [==============================] - 369s 5us/sample\n",
      "\t... ensemble 1\n",
      "73927845/73927845 [==============================] - 344s 5us/sample\n",
      "\t... ensemble 2\n",
      "73927845/73927845 [==============================] - 357s 5us/sample\n",
      "\t... ensemble 3\n",
      "73927845/73927845 [==============================] - 344s 5us/sample\n",
      "\t... ensemble 4\n",
      "73927845/73927845 [==============================] - 346s 5us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Deconvolution:  33%|█████████████████████▋                                           | 2/6 [45:14<1:38:18, 1474.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Deconvolution for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 17), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0, 'decon_id': 1}\n",
      "Using deconvolution model Global_EXC_30Hz_smoothing50ms\n",
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 50 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 30 Hz\n",
      "Given argument traces contains 1586 neurons and 66613 frames.\n",
      "Noise levels (mean, std; in standard units): 0.93, 0.21\n",
      "\n",
      "Predictions for noise level 2:\n",
      "\t... ensemble 0\n",
      "105648218/105648218 [==============================] - 538s 5us/sample\n",
      "\t... ensemble 1\n",
      "105648218/105648218 [==============================] - 497s 5us/sample\n",
      "\t... ensemble 2\n",
      "105648218/105648218 [==============================] - 493s 5us/sample\n",
      "\t... ensemble 3\n",
      "105648218/105648218 [==============================] - 495s 5us/sample\n",
      "\t... ensemble 4\n",
      "105648218/105648218 [==============================] - 496s 5us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Deconvolution:  50%|███████████████████████████████▌                               | 3/6 [1:58:58<2:21:04, 2821.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Deconvolution for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 20), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0, 'decon_id': 1}\n",
      "Using deconvolution model Global_EXC_30Hz_smoothing50ms\n",
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 50 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 30 Hz\n",
      "Given argument traces contains 1323 neurons and 49359 frames.\n",
      "Noise levels (mean, std; in standard units): 1.13, 0.2\n",
      "\n",
      "Predictions for noise level 2:\n",
      "\t... ensemble 0\n",
      "65252598/65252598 [==============================] - 302s 5us/sample\n",
      "\t... ensemble 1\n",
      "65252598/65252598 [==============================] - 302s 5us/sample\n",
      "\t... ensemble 2\n",
      "65252598/65252598 [==============================] - 305s 5us/sample\n",
      "\t... ensemble 3\n",
      "65252598/65252598 [==============================] - 302s 5us/sample\n",
      "\t... ensemble 4\n",
      "65252598/65252598 [==============================] - 304s 5us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\t... ensemble 0\n",
      "49359/49359 [==============================] - 0s 6us/sample\n",
      "\t... ensemble 1\n",
      "49359/49359 [==============================] - 0s 6us/sample\n",
      "\t... ensemble 2\n",
      "49359/49359 [==============================] - 0s 6us/sample\n",
      "\t... ensemble 3\n",
      "49359/49359 [==============================] - 0s 6us/sample\n",
      "\t... ensemble 4\n",
      "49359/49359 [==============================] - 0s 6us/sample\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Deconvolution:  67%|██████████████████████████████████████████                     | 4/6 [2:27:14<1:19:13, 2376.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Deconvolution for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 23), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0, 'decon_id': 1}\n",
      "Using deconvolution model Global_EXC_30Hz_smoothing50ms\n",
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 50 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 30 Hz\n",
      "Given argument traces contains 1230 neurons and 34598 frames.\n",
      "Noise levels (mean, std; in standard units): 1.05, 0.19\n",
      "\n",
      "Predictions for noise level 2:\n",
      "\t... ensemble 0\n",
      "42555540/42555540 [==============================] - 198s 5us/sample\n",
      "\t... ensemble 1\n",
      "42555540/42555540 [==============================] - 197s 5us/sample\n",
      "\t... ensemble 2\n",
      "42555540/42555540 [==============================] - 198s 5us/sample\n",
      "\t... ensemble 3\n",
      "42555540/42555540 [==============================] - 198s 5us/sample\n",
      "\t... ensemble 4\n",
      "42555540/42555540 [==============================] - 198s 5us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Deconvolution:  83%|██████████████████████████████████████████████████████▏          | 5/6 [2:44:58<31:43, 1903.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Deconvolution for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 31), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0, 'decon_id': 1}\n",
      "Using deconvolution model Global_EXC_30Hz_smoothing50ms\n",
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 30Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 50 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 30 Hz\n",
      "Given argument traces contains 901 neurons and 26546 frames.\n",
      "Noise levels (mean, std; in standard units): 1.29, 0.21\n",
      "\n",
      "Predictions for noise level 2:\n",
      "\t... ensemble 0\n",
      "23917946/23917946 [==============================] - 115s 5us/sample\n",
      "\t... ensemble 1\n",
      "23917946/23917946 [==============================] - 109s 5us/sample\n",
      "\t... ensemble 2\n",
      "23917946/23917946 [==============================] - 108s 5us/sample\n",
      "\t... ensemble 3\n",
      "23917946/23917946 [==============================] - 108s 5us/sample\n",
      "\t... ensemble 4\n",
      "23917946/23917946 [==============================] - 108s 5us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deconvolution: 100%|█████████████████████████████████████████████████████████████████| 6/6 [2:54:49<00:00, 1748.29s/it]\n"
     ]
    }
   ],
   "source": [
    "common_img.Deconvolution().populate({'username': 'hheise', 'decon_id': 1}, display_progress=True, reserve_jobs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d885b",
   "metadata": {},
   "source": [
    "## Step 4.5: Activity Statistics\n",
    "\n",
    "Lastly, `ActivityStatistics()` computes and stores some analysis of the deconvolution traces like number of spikes, average spike rate, and number of events (threshold crossings, one event can be more than one spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8502e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ActivityStatistics: 100%|████████████████████████████████████████████████████████████████| 6/6 [00:17<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "common_img.ActivityStatistics().populate({'username': 'hheise'}, reserve_jobs=True, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "spikerate = (common_img.ActivityStatistics.ROI() & restrictions).fetch('rate_spikes')\n",
    "\n",
    "plt.hist(spikerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0797aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
