{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3881c4",
   "metadata": {},
   "source": [
    "# Running the CaImAn pipeline with DataJoint\n",
    "\n",
    "This notebook provides the necessary steps to run the CaImAn segmentation pipeline on the DataJoint database. \n",
    "\n",
    "<b>IMPORTANT: </b> This notebook assumes that all user-specific data (for `common_exp.Session()`, `common_img.Scan()` and `common_img.RawImagingFile()`) have been entered elsewhere! Once this is done, the pipeline is common for all useres and can be executed here.\n",
    "\n",
    "Because this notebook is using CaImAn's functions, it needs to be run inside the Caiman environment. If you are in the Datajoint environment, the tables will still load and you can view the data, but trying to call Caiman-specific functions will result in `ModuleNotFoundErrors` or `NameErrors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f038c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting hheise@130.60.53.48:3306\n"
     ]
    }
   ],
   "source": [
    "# Importing datajoint tables\n",
    "import sys\n",
    "sys.path.append(\"..\\\\\")\n",
    "\n",
    "import login\n",
    "login.connect()\n",
    "from schema import common_img, common_exp\n",
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2406337",
   "metadata": {},
   "source": [
    "## Step 0: Check that the manual data is there\n",
    "\n",
    "Before running the actual Caiman pipeline, the manual data has to be entered into the database so that Datajoint knows where to start looking for the data. You can do this by checking the `Session()` and `Scan()` tables, with restrictions to only get the sessions you are about to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff07d68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    <style type=\"text/css\">\n",
       "        .Relation{\n",
       "            border-collapse:collapse;\n",
       "        }\n",
       "        .Relation th{\n",
       "            background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "            font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "        }\n",
       "        .Relation td{\n",
       "            padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "        }\n",
       "        .Relation tr:nth-child(odd){\n",
       "            background: #ffffff;\n",
       "        }\n",
       "        .Relation tr:nth-child(even){\n",
       "            background: #f3f1ff;\n",
       "        }\n",
       "        /* Tooltip container */\n",
       "        .djtooltip {\n",
       "        }\n",
       "        /* Tooltip text */\n",
       "        .djtooltip .djtooltiptext {\n",
       "            visibility: hidden;\n",
       "            width: 120px;\n",
       "            background-color: black;\n",
       "            color: #fff;\n",
       "            text-align: center;\n",
       "            padding: 5px 0;\n",
       "            border-radius: 6px;\n",
       "            /* Position the tooltip text - see examples below! */\n",
       "            position: absolute;\n",
       "            z-index: 1;\n",
       "        }\n",
       "        #primary {\n",
       "            font-weight: bold;\n",
       "            color: black;\n",
       "        }\n",
       "        #nonprimary {\n",
       "            font-weight: normal;\n",
       "            color: white;\n",
       "        }\n",
       "\n",
       "        /* Show the tooltip text when you mouse over the tooltip container */\n",
       "        .djtooltip:hover .djtooltiptext {\n",
       "            visibility: visible;\n",
       "        }\n",
       "    </style>\n",
       "    \n",
       "    <b>Information about the session and experimental setup</b>\n",
       "        <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "        <table border=\"1\" class=\"Relation\">\n",
       "            <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">username</p>\n",
       "                            <span class=\"djtooltiptext\">Unique username, should be the UZH shortname</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">mouse_id</p>\n",
       "                            <span class=\"djtooltiptext\">ID of mouse (unique per investigator)</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">day</p>\n",
       "                            <span class=\"djtooltiptext\">Date of the experimental session (YYYY-MM-DD)</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">session_num</p>\n",
       "                            <span class=\"djtooltiptext\">Counter of experimental sessions on the same day (base 1)</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">session_id</p>\n",
       "                            <span class=\"djtooltiptext\">Unique identifier</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">session_path</p>\n",
       "                            <span class=\"djtooltiptext\">Relative path of this session on the Neurophysiology-Storage1 server</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">session_counter</p>\n",
       "                            <span class=\"djtooltiptext\">Overall counter of all sessions across mice (base 0)</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">experimenter</p>\n",
       "                            <span class=\"djtooltiptext\">Who actually performed the experiment, must be a username from Investigator()</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">anesthesia</p>\n",
       "                            <span class=\"djtooltiptext\">Anesthesia short name</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">setup</p>\n",
       "                            <span class=\"djtooltiptext\">Unique name of the setup</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">task</p>\n",
       "                            <span class=\"djtooltiptext\">Unique name of the task</span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">session_notes</p>\n",
       "                            <span class=\"djtooltiptext\">description of important things that happened</span>\n",
       "                        </div> </th> </tr> </thead>\n",
       "            <tbody> <tr>  </tr> </tbody>\n",
       "        </table>\n",
       "        \n",
       "        <p>Total: 0</p></div>\n",
       "        "
      ],
      "text/plain": [
       "*username    *mouse_id    *day    *session_num   session_id     session_path   session_counte experimenter   anesthesia     setup     task     session_notes \n",
       "+----------+ +----------+ +-----+ +------------+ +------------+ +------------+ +------------+ +------------+ +------------+ +-------+ +------+ +------------+\n",
       "\n",
       " (Total: 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restrict the query with some parameters\n",
    "restrictions = dict(username='hheise', mouse_id=69)\n",
    "\n",
    "# Show the entries of the tables that fit these restrictions\n",
    "(common_exp.Session() & restrictions & \"day = '2021-08-11'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c946733",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.Scan() & restrictions & \"day = '2021-07-16'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b76a1",
   "metadata": {},
   "source": [
    "## Run the pipeline: Populate Computed Tables\n",
    "\n",
    "If the above queries returned the right data, we are good to go for our pipeline!\n",
    "\n",
    "Datajoint's pipeline consists mainly of consecutive `Computed` tables, that can be filled in automatically with the processed data. This \"filling in\" is done by calling each table's `populate()` function. A detailed explanation can be found [here](https://docs.datajoint.org/python/computation/01-autopopulate.html#populate), but in short, Datajoint looks up all possible combinations of entries in upstream tables and runs it's `make()` function (where the actual computation for the table is located) for each of these combinations. This makes it possible for example to run and store the Caiman Segmentation for one session and several sets of parameters.\n",
    "\n",
    "Datajoint offers us a basic way to visualize the tables and its dependencies. The color of each table name shows its type (green = `Manual`, grey = `Lookup`, blue = `Imported`, red = `Computed`). The pipeline is visible as the \"string\" of red `Computed` tables going from `ScanInfo()` to `Deconvolution()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Entity Relationship Diagram (ERD) of a schema.\n",
    "### If the error '[WinError 2] \"dot\" not found in path.' occurs, you have to install graphviz by running \n",
    "### \"conda install -c conda-forge python-graphviz\" in an anaconda prompt with your environment active.\n",
    "%matplotlib qt\n",
    "dj.ERD(common_img).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a367ee",
   "metadata": {},
   "source": [
    "## Step 0.5: Populate RawImagingFile()\n",
    "\n",
    "Before running the actual pipeline, we have to find the raw imaging files from the current scan session. Datajoint does this automatically, but makes two assumptions that each user has to ensure is true:\n",
    "\n",
    "1. All imaging files for each session have to be located in the directory specified in `session_path` on the user's data directory on the Wahl server\n",
    "2. All imaging files have to follow the naming convention(s) specified in the user's `gui_params.yaml` file under the `scientifica_file` entry\n",
    "\n",
    "Datajoint then goes through the `session_path` directory and all its subfolders and finds all files that fit the pattern. The pipeline will then be performed on these files.\n",
    "\n",
    "**Important:** The database only stores the **file paths** of raw files, not the files themselves, due to memory constraints. It cannot keep track of changes to this file path after you entered the data into `RawImagingFile()`. This means that if you move the files to a different location, or rename one of the upstream folders, you will break the file path and Datajoint will not find the files anymore. In this case, if you want to perform processing steps where Datajoint needs access to the raw files (like motion correction or segmentation), you will have to update `session_path` of these files before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the table\n",
    "common_img.RawImagingFile().populate(display_progress=True)\n",
    "\n",
    "# View the resulting entry\n",
    "common_img.RawImagingFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638eb3b1",
   "metadata": {},
   "source": [
    "## Step 1: Populate ScanInfo()\n",
    "\n",
    "The first `Computed` table we have to populate is `ScanInfo()`. This table stores hardware and software settings of the scan that can be automatically read from the raw TIFF files coming from ScanImage, like frame rate, Pockels cell power, or XYZ stage position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the table\n",
    "common_img.ScanInfo().populate({'username': 'hheise'}, reserve_jobs=False, display_progress=True)\n",
    "\n",
    "# View the resulting entry\n",
    "common_img.ScanInfo() & restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86541ec0",
   "metadata": {},
   "source": [
    "## Step 2: Motion Correction\n",
    "\n",
    "Next up is the motion correction. Here we need a parameter set for the first time. The parameters for the motion correction is stored in the `Manual` table `MotionParameter()`. Because the motion correction parameters are generally transferable across brain regions and mice, we have a single table storing all sets. Each has a short description about the condition for which this set is appropriate (e.g. which Zoom setting, low or high movement artefacts, etc).\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.MotionParameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775b04b",
   "metadata": {},
   "source": [
    "If we want to add a parameter set, we can do this manually by entering a new dict into the table. This function also automatically saves manual each entry in a backup YAML file, in case the database gets corrupted and has to be restored.\n",
    "\n",
    "If you do not want to change each parameter, you can comment them out. In this case, the default value from `motion_id = 0` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580eff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_entry = dict(\n",
    "                 motion_id=0,\n",
    "                 motion_shortname='default',\n",
    "                 motion_description='Default values that work well from Hendriks previous experience. Uses piecewise-rigid correction.',\n",
    "                 crop_left=12,\n",
    "                 crop_right=12,\n",
    "                 offset=220,\n",
    "                 max_shift=50,\n",
    "                 stride_mc=150,\n",
    "                 overlap_mc=30,\n",
    "                 pw_rigid=1,\n",
    "                 max_dev_rigid=3,\n",
    "                 border_nan=0,\n",
    "                 n_iter_rig=2,\n",
    "                 nonneg_movie=1,\n",
    "                )\n",
    "\n",
    "common_img.MotionParameter().helper_insert1(new_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c1526",
   "metadata": {},
   "source": [
    "Once we have decided which parameter set to use, we can call the `populate()` function of the `MotionCorrection()` table with the respective restriction.\n",
    "\n",
    "`MotionCorrection()` does several things for each scan:\n",
    "1. Download the raw TIFF files for this scan into a temporary cache directory on the local machine.\n",
    "2. Do some preprocessing necessary for files from the H37R Scientifica: Align odd and even scan lines, crop the left and right borders to avoid edge artifacts, and apply a fixed offset (see `MotionParameter()` entry) to avoid mean negative pixel values that can mess with the CNMF algorithm.\n",
    "3. Perform the actual motion correction.\n",
    "4. Extract the calculated pixel shifts from the motion correction and store it in the database.\n",
    "5. Calculate and save some quality control metrics (e.g. correlation with template, means and standard deviations)\n",
    "6. Delete the memory-mapped files from the disk.\n",
    "\n",
    "As you can see, we delete the memory-mapped file directly after the motion correction. This is because we only store the pixel shifts in the database, and not the entire file. When we need the motion-corrected memory-mapped movie, we can recreate it easily by applying the pixel shifts to the corrected raw TIFF files. This is much faster than running the entire motion correction algorithm again, and it saves tons of disk space on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the populate() function, restricting it if necessary for only one set of parameters\n",
    "common_img.MotionCorrection().populate({'username': 'hheise', 'motion_id': 0, 'mouse_id': 69}, display_progress=True)\n",
    "\n",
    "# View the resulting entry\n",
    "common_img.MotionCorrection() & restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ecb18c",
   "metadata": {},
   "source": [
    "## Step 2.5: Quality control of MotionCorrection()\n",
    "\n",
    "To be able to quickly judge whether the motion correction was sufficiently effective without having to export the entire movie, we are computing some metrics that can help evaluate the motion correction. This table is not necessary for the next step of the pipeline, but is nice to have.\n",
    "\n",
    "These are Z-projections where each pixel has a certain value dependent on the time course of this pixel. The Z-projections we calculate are:\n",
    "- Average intensity\n",
    "- Local correlation (intensity correlation between each pixel and its 8 neighbors)\n",
    "- Standard deviation, minimum and maximum\n",
    "- 99.9th percentile\n",
    "- average intensity of all pixels over time (makes slow intensity changes visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35292074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "QualityControl:   0%|                                                                                                                                                                       | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating QualityControl for key: {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 10), 'session_num': 1, 'motion_id': 0}.\n",
      "\tFound existing mmap file, skipping motion correction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QualityControl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [23:04<00:00, 1384.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting mmap file failed, file is being used: C:\\Users\\hheise\\Datajoint\\temp\\20220705_094746\\mmap__d1_488_d2_488_d3_1_order_C_frames_36753_.mmap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "common_img.QualityControl().populate({'username': 'hheise'}, reserve_jobs=False, display_progress=True)\n",
    "\n",
    "# common_img.QualityControl() & restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f1cf4",
   "metadata": {},
   "source": [
    "## Step 3: Segmentation\n",
    "\n",
    "This is the actual CaImAn segmentation algorithm. It performs component detection, evaluation and dF/F computation steps. For this we also need a parameter set, which are stored in `CaimanParameter()`. Because these parameters are much more dependent on brain region and the specific FOV of the mouse (cell density, fluorescence intensity, etc.), each mouse gets its own list of parameter sets. If you want to use the same set of parameters for several mice, the set has to be entered into the database for each mouse. \n",
    "\n",
    "This means that instead of only one ID (like `motion_id`), `CaimanParameter()` needs two, `caiman_id` and `mouse_id`. This also means that several mice can have a set with `caiman_id=0`, but that contains slightly different parameters. This can be useful to store sets with different criteria. `caiman_id=0` can be a strict parameter set that only finds 100% certain neurons, and `caiman_id=1` could be more lenient, also accepting components that are not 100% neurons. The exact values in each set will vary between mice, but the \"meaning\" of each `caiman_id` is conserved across mice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b76a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.CaimanParameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc6898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_entry = dict(\n",
    "            # Identifiers\n",
    "             username = 'hheise',\n",
    "             mouse_id = 93,\n",
    "             caiman_id = 0,\n",
    "            # Component detection parameters\n",
    "             p = 1,\n",
    "             nb = 2,\n",
    "             merge_thr = 0.75,\n",
    "             rf = 25,\n",
    "             stride_cnmf = 6,\n",
    "             k = 18,\n",
    "             g_sig = 4,\n",
    "             method_init = 'greedy_roi',\n",
    "             ssub = 2,\n",
    "             tsub = 2,\n",
    "            # Evaluation parameters\n",
    "             snr_lowest = 5.0,\n",
    "             snr_thr = 9.0,\n",
    "             rval_lowest = -1.0,\n",
    "             rval_thr = 0.85,\n",
    "             cnn_lowest = 0.1,\n",
    "             cnn_thr = 0.9,\n",
    "            # dF/F parameters\n",
    "             flag_auto = 1,\n",
    "             quantile_min = 8,\n",
    "             frame_window = 2000,\n",
    "            )\n",
    "\n",
    "common_img.CaimanParameter().helper_insert1(new_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3968a5",
   "metadata": {},
   "source": [
    "The `Segmentation()` table performs the complete CaImAn pipeline. Results are not stored in an HDF5 file (although you can save them as such), but stored directly in the database. Each accepted component and their attributes (spatial and temporal components, dF/F, evaluation scores) are stored in the `Segmentation.ROI()` Part-table.\n",
    "\n",
    "`Segmentation().populate()` has two additional arguments, which control whether a FOV overview with outlined components should be saved after source detection and after evaluation (`save_overviews`) and whether the results should be stored in a traditional `cnm_results.hdf5` file in addition to the database entries (`save_results`). If `True`, these results are saved in the session folder on the Wahl server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d25f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Segmentation:   0%|                                                                                                                                                                        | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Segmentation for {'username': 'hheise', 'mouse_id': 69, 'day': datetime.date(2021, 3, 10), 'session_num': 1, 'motion_id': 0, 'caiman_id': 0}.\n",
      "MemoryMappedFile entry already exists, skipping motion correction.\n",
      "\tStart running CNMF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Component 14 is only active jointly with neighboring components. Space correlation calculation might be unreliable.\n",
      "WARNING:root:Component 25 is only active jointly with neighboring components. Space correlation calculation might be unreliable.\n",
      "WARNING:root:Component 38 is only active jointly with neighboring components. Space correlation calculation might be unreliable.\n",
      "WARNING:root:Component 33 is only active jointly with neighboring components. Space correlation calculation might be unreliable.\n",
      "WARNING:root:Component 29 is only active jointly with neighboring components. Space correlation calculation might be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING MODEL:C:\\Users\\hheise\\caiman_data\\model\\cnn_model.json\n",
      "1874/1874 [==============================] - 1s 412us/sample\n",
      "Saving plots at: F:\\Batch5\\M69\\20210310\n",
      "\tStart computing dF/F...\n"
     ]
    }
   ],
   "source": [
    "common_img.Segmentation().populate({'username': 'hheise', 'caiman_id': 0}, make_kwargs=dict(save_overviews=True, save_results=False), display_progress=True)\n",
    "\n",
    "common_img.Segmentation() & restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.Segmentation.ROI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959377ac",
   "metadata": {},
   "source": [
    "## Step 4: Deconvolution\n",
    "\n",
    "The last step of our pipeline is Peter's CASCADE deconvolution. For this we need to decide which pretrained model to use. The best four are included in `DeconvolutionModel()`, with different parameters that influence the deconvolution [described here](https://github.com/HelmchenLabSoftware/Cascade#what-does-the-smoothing-parameter-for-the-models-mean):\n",
    "\n",
    "- Smoothing window: standard deviation of smoothing Gaussian kernel around ground truth in milliseconds. Depends on the frame rate of the recording. At 30 Hz, a smoothing of 50 ms is recommended. If imaging quality is bad, increase smoothing window.\n",
    "- Causal kernel: By default, the ground truth is smoothed symmetrically in time with a Gaussian kernel. This might predict some activity before the actual calcium transient. If time resolution is very important, e.g. with stimulus-triggered activity, a causal kernel almost exclusively predicts activity after the calcium event. However, a causal kernel is more error-prone and might predict events on noise, so should only be chosen if really necessary.\n",
    "\n",
    "Currently, four models are included in `DeconvolutionModel()`: 50ms, 100ms, causal kernel, Gaussian kernel. Additional models can be downloaded [here](https://github.com/HelmchenLabSoftware/Cascade/blob/master/Pretrained_models/available_models.yaml) and added to the table. In general, the ensemble models trained on many different datasets of excitatory activity (labeled `Global_EXC_`) perform better than single models and should be preferred.\n",
    "\n",
    "When populating `Deconvolution()`, restrict the algorithm to only use the most suitable model.\n",
    "\n",
    "The results of of the deconvolution are stored in the `Deconvolution.ROI()` part table again, with the same ID as in `Segmentation.ROI()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.DeconvolutionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d70e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_img.Deconvolution().populate({'username': 'hheise', 'decon_id': 1}, display_progress=True, reserve_jobs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d885b",
   "metadata": {},
   "source": [
    "## Step 4.5: Activity Statistics\n",
    "\n",
    "Lastly, `ActivityStatistics()` computes and stores some analysis of the deconvolution traces like number of spikes, average spike rate, and number of events (threshold crossings, one event can be more than one spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8502e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_img.ActivityStatistics().populate({'username': 'hheise'}, reserve_jobs=True, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "spikerate = (common_img.ActivityStatistics.ROI() & restrictions).fetch('rate_spikes')\n",
    "\n",
    "plt.hist(spikerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0797aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
